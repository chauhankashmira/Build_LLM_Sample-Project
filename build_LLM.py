
#Tokenizer
from transformers import AutoTokenizer 
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")


input_tokens = tokenizer ("Kash is the best", return_tensor="pt")
input_tokens # run this code first to get tokens then write below code


#Load the Model
import torch 
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("google/gemma-3-1b-it", torch_dtype=torch.bfloat16) # run the code to load the model


#LINEARS: Output Token (predictions)
out = model (input_ids=input_tokens["input_ids"]) #input_ids are generated by input_token in tokenizer step



#Generate more predictions with generate function. When use generate function, it gives you the predictions which have high probabilities
gen_out = model.generate(input_ids=input_tokens["input_ids"], max_new_tokens=100)
gen_out #run the code


#De-tokentize
tokenizer.batch_decode(gen_out) #then run the code here





